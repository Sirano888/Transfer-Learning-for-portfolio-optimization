{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-mOsNLoUtiu"
      },
      "outputs": [],
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Paramètres globaux (réutilisables pour tous les pays)\n",
        "risk_free_rate = 0.05 / 252  # Taux sans risque journalier (5% annuel)\n",
        "source_start_date = \"2000-02-01\"\n",
        "source_end_date = \"2020-02-01\"\n",
        "target_start_date = \"2015-02-01\"\n",
        "target_end_date = \"2020-02-01\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Royaume-Uni"
      ],
      "metadata": {
        "id": "v6eXq-Y8kOS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import pearsonr\n",
        "import random\n",
        "\n",
        "# Global parameters\n",
        "risk_free_rate = 0.05 / 252  # Daily risk-free rate (5% annual)\n",
        "source_start_date = \"2000-02-01\"\n",
        "source_end_date = \"2020-02-01\"\n",
        "target_start_date = \"2015-02-01\"\n",
        "target_end_date = \"2020-02-01\"\n",
        "test_start_date = \"2020-02-01\"\n",
        "test_end_date = \"2021-09-01\"\n",
        "lambda_reg = 0.2  # Regularization parameter\n",
        "\n",
        "# Fixed target assets (e.g., top 10 from FTSE 100 for UK)\n",
        "target_assets = ['AZN.L', 'SHEL.L', 'HSBA.L', 'ULVR.L', 'BHP.L', 'REL.L', 'BATS.L', 'BP.L', 'LSEG.L', 'RR.L']\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to get S&P 500 tickers with their addition date\n",
        "def get_sp500_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "\n",
        "    # Convert the \"Date added\" column to datetime\n",
        "    sp500_df['Date added'] = pd.to_datetime(sp500_df['Date added'], errors='coerce')\n",
        "\n",
        "    # Filter tickers added after the year 2000\n",
        "    sp500_df = sp500_df[sp500_df['Date added'] > datetime(2000, 1, 1)]\n",
        "\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers added after 2000\n",
        "def select_random_tickers(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "# Function to get S&P 500 tickers\n",
        "def get_sp500_tickers2():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers\n",
        "def select_random_tickers2(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "\n",
        "# Function to download excess returns\n",
        "def download_excess_returns(tickers, start_date, end_date):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        if not stock_data.empty:\n",
        "            price_column = 'Adj Close' if 'Adj Close' in stock_data.columns else 'Close'\n",
        "            stock_data['Excess Returns'] = stock_data[price_column].pct_change() - risk_free_rate\n",
        "            data[ticker] = stock_data['Excess Returns']\n",
        "    return pd.DataFrame(data).dropna()  # Drop rows with missing values\n",
        "\n",
        "\n",
        "# Function to optimize the source portfolio (phi_S)\n",
        "def optimize_source_portfolio(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_source.x\n",
        "\n",
        "def optimize_source_portfolio2(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_source.fun\n",
        "\n",
        "# Function to optimize the target portfolio with regularization (phi_T)\n",
        "def optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_target.x, -result_target.fun\n",
        "\n",
        "def optimize_target_portfolio2(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_target.fun\n",
        "\n",
        "# Function to compute transfer risk\n",
        "# Function to compute R1 (Transfer Risk)\n",
        "def compute_R1(phi, mu_T, Sigma_T):\n",
        "    numerator = np.dot(mu_T, phi)\n",
        "    denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "    ratio = numerator / denominator\n",
        "    R1 = 1.0 / ratio\n",
        "    return R1\n",
        "\n",
        "def compute_transfer_risk(phi_T, source_data, target_test_data):\n",
        "    mu_S = np.array(source_data.mean())\n",
        "    Sigma_S = np.array(source_data.cov())\n",
        "    mu_T_test = np.array(target_test_data.mean())\n",
        "    Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "    # Compute R1 (inverse of source portfolio Sharpe ratio)\n",
        "    R1 = compute_R1(optimize_source_portfolio(mu_S, Sigma_S), mu_S, Sigma_S)\n",
        "\n",
        "    # Compute R2 (Wasserstein-2 distance between source and target distributions)\n",
        "    R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "    return R1 + R2\n",
        "\n",
        "\n",
        "\n",
        "# Main experiment loop\n",
        "n_experiments = 10  # Reduced to 100 for faster testing; increase to 1000 for final results\n",
        "sharpe_ratios_transfer = []\n",
        "sharpe_ratios_direct = []\n",
        "transfer_risks = []\n",
        "\n",
        "# Download target data\n",
        "target_data = download_excess_returns(target_assets, target_start_date, test_end_date)\n",
        "target_train_data = target_data.loc[:target_end_date]  # Training data up to February 2020\n",
        "target_test_data = target_data.loc[test_start_date:]   # Testing data from February 2020 onwards\n",
        "\n",
        "# Compute target mean and covariance for training data\n",
        "mu_T = np.array(target_train_data.mean())\n",
        "Sigma_T = np.array(target_train_data.cov())\n",
        "\n",
        "for _ in range(n_experiments):\n",
        "    try:\n",
        "        # Randomly select source assets (10 random S&P 500 stocks)\n",
        "        source_assets = select_random_tickers(n=10)\n",
        "        print(\"Source Assets:\", source_assets)\n",
        "\n",
        "        # Generate source data\n",
        "        source_data = download_excess_returns(source_assets, source_start_date, source_end_date)\n",
        "        if not isinstance(source_data, pd.DataFrame) or source_data.shape[1] != 10:\n",
        "            print(\"Skipping experiment due to missing data for some source assets.\")\n",
        "            continue\n",
        "\n",
        "        # Optimize source portfolio (phi_S)\n",
        "        mu_S = np.array(source_data.mean())\n",
        "        Sigma_S = np.array(source_data.cov())\n",
        "        phi_S = optimize_source_portfolio(mu_S, Sigma_S)\n",
        "\n",
        "        # Optimize target portfolio with transfer learning (phi_T)\n",
        "        phi_T, sharpe_ratio_transfer = optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg)\n",
        "\n",
        "        # Optimize target portfolio with direct learning (phi_T_direct)\n",
        "        phi_T_direct = optimize_source_portfolio(mu_T, Sigma_T)\n",
        "\n",
        "        # Compute target mean and covariance for testing data\n",
        "        mu_T_test = np.array(target_test_data.mean())\n",
        "        Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "        # Compute Sharpe ratios on testing data (inverser compute_R1 pour obtenir le vrai ratio)\n",
        "        #sharpe_ratio_transfer_test = 1.0 / compute_R1(phi_S, mu_T_test, Sigma_T_test)\n",
        "        #sharpe_ratio_direct_test = 1.0 / compute_R1(phi_T_direct, mu_T_test, Sigma_T_test)\n",
        "        #phiTwhatever, sharpe_ratio_transfer_test = optimize_target_portfolio(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_transfer_test = optimize_target_portfolio2(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_direct_test= optimize_source_portfolio2(mu_T_test, Sigma_T_test)\n",
        "\n",
        "        # Compute transfer risk\n",
        "        R1 = compute_R1(phi_S, mu_S, Sigma_S)  # Correction : calculer R1 avec phi_S\n",
        "        R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "        transfer_risk = R1 + R2\n",
        "\n",
        "        # Store results\n",
        "        sharpe_ratios_transfer.append(sharpe_ratio_transfer)\n",
        "        sharpe_ratios_direct.append(sharpe_ratio_direct_test)\n",
        "        transfer_risks.append(transfer_risk)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in experiment {_}: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# Compute correlation between Sharpe ratios and transfer risks\n",
        "if len(sharpe_ratios_transfer) > 1 and len(transfer_risks) > 1:\n",
        "    correlation, p_value = pearsonr(sharpe_ratios_transfer, transfer_risks)\n",
        "    print(\"Correlation between Sharpe Ratio (Transfer) and Transfer Risk:\", correlation)\n",
        "    print(\"p-value:\", p_value)\n",
        "else:\n",
        "    print(\"Not enough experiments completed to compute correlation.\")\n",
        "    print(transfer_risks)\n",
        "    print(sharpe_ratios_transfer)\n",
        "\n",
        "    print(sharpe_ratios_direct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAzXgxCPmEJL",
        "outputId": "4a0cb9cc-1b32-48a0-a2b9-7541255b5bbe"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['POOL', 'DD', 'NCLH', 'TXN', 'AME', 'IDXX', 'IT', 'LULU', 'AKAM', 'BXP']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['MCHP', 'MAA', 'RCL', 'CHTR', 'WEC', 'FANG', 'CE', 'LYB', 'WELL', 'MHK']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['LVS', 'DVN', 'FOXA', 'EL', 'BWA', 'MOH', 'POOL', 'PTC', 'META', 'AMT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['FIS', 'HII', 'AVB', 'JBL', 'MET', 'KDP', 'URI', 'IQV', 'CTSH', 'UDR']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['LKQ', 'GM', 'NVR', 'VICI', 'AMZN', 'EQR', 'IEX', 'NWSA', 'COR', 'TER']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['HWM', 'LYV', 'TSLA', 'AXON', 'PKG', 'STLD', 'NXPI', 'DG', 'TRGP', 'EQT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['ARE', 'TRMB', 'AKAM', 'BKR', 'IT', 'CME', 'EQT', 'JNPR', 'NRG', 'EXPD']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['IEX', 'ARE', 'INVH', 'PKG', 'PLTR', 'CTAS', 'LRCX', 'TTWO', 'MET', 'LKQ']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['PLTR']: YFPricesMissingError('possibly delisted; no price data found  (1d 2000-02-01 -> 2020-02-01) (Yahoo error = \"Data doesn\\'t exist for startDate = 949381200, endDate = 1580533200\")')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping experiment due to missing data for some source assets.\n",
            "Source Assets: ['CTRA', 'CRWD', 'WRB', 'ULTA', 'LW', 'KDP', 'FAST', 'FOXA', 'CRL', 'FRT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['PLTR']: YFPricesMissingError('possibly delisted; no price data found  (1d 2000-02-01 -> 2020-02-01) (Yahoo error = \"Data doesn\\'t exist for startDate = 949381200, endDate = 1580533200\")')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Assets: ['PLTR', 'CFG', 'DD', 'RSG', 'POOL', 'KVUE', 'CE', 'ELV', 'MTCH', 'SBUX']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['KVUE']: YFPricesMissingError('possibly delisted; no price data found  (1d 2000-02-01 -> 2020-02-01) (Yahoo error = \"Data doesn\\'t exist for startDate = 949381200, endDate = 1580533200\")')\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping experiment due to missing data for some source assets.\n",
            "Correlation between Sharpe Ratio (Transfer) and Transfer Risk: 0.6700660013005417\n",
            "p-value: 0.06903647951625742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brésil"
      ],
      "metadata": {
        "id": "w7q6SsgMkYJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import pearsonr\n",
        "import random\n",
        "\n",
        "# Global parameters\n",
        "risk_free_rate = 0.05 / 252  # Daily risk-free rate (5% annual)\n",
        "source_start_date = \"2000-02-01\"\n",
        "source_end_date = \"2020-02-01\"\n",
        "target_start_date = \"2015-02-01\"\n",
        "target_end_date = \"2020-02-01\"\n",
        "test_start_date = \"2020-02-01\"\n",
        "test_end_date = \"2021-09-01\"\n",
        "lambda_reg = 0.2  # Regularization parameter\n",
        "\n",
        "# Fixed target assets (e.g., top 10 from BOVESPA)\n",
        "target_assets= ['PETR4.SA', 'VALE3.SA', 'ITUB4.SA', 'BBDC4.SA', 'ABEV3.SA',\n",
        "                'BBAS3.SA', 'B3SA3.SA', 'WEGE3.SA', 'EQTL3.SA', 'SUZB3.SA']\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to get S&P 500 tickers with their addition date\n",
        "def get_sp500_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "\n",
        "    # Convert the \"Date added\" column to datetime\n",
        "    sp500_df['Date added'] = pd.to_datetime(sp500_df['Date added'], errors='coerce')\n",
        "\n",
        "    # Filter tickers added after the year 2000\n",
        "    sp500_df = sp500_df[sp500_df['Date added'] > datetime(2000, 1, 1)]\n",
        "\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers added after 2000\n",
        "def select_random_tickers(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "# Function to get S&P 500 tickers\n",
        "def get_sp500_tickers2():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers\n",
        "def select_random_tickers2(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "\n",
        "# Function to download excess returns\n",
        "def download_excess_returns(tickers, start_date, end_date):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        if not stock_data.empty:\n",
        "            price_column = 'Adj Close' if 'Adj Close' in stock_data.columns else 'Close'\n",
        "            stock_data['Excess Returns'] = stock_data[price_column].pct_change() - risk_free_rate\n",
        "            data[ticker] = stock_data['Excess Returns']\n",
        "    return pd.DataFrame(data).dropna()  # Drop rows with missing values\n",
        "\n",
        "\n",
        "# Function to optimize the source portfolio (phi_S)\n",
        "def optimize_source_portfolio(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_source.x\n",
        "\n",
        "def optimize_source_portfolio2(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_source.fun\n",
        "\n",
        "# Function to optimize the target portfolio with regularization (phi_T)\n",
        "def optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_target.x, -result_target.fun\n",
        "\n",
        "def optimize_target_portfolio2(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_target.fun\n",
        "\n",
        "# Function to compute transfer risk\n",
        "# Function to compute R1 (Transfer Risk)\n",
        "def compute_R1(phi, mu_T, Sigma_T):\n",
        "    numerator = np.dot(mu_T, phi)\n",
        "    denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "    ratio = numerator / denominator\n",
        "    R1 = 1.0 / ratio\n",
        "    return R1\n",
        "\n",
        "def compute_transfer_risk(phi_T, source_data, target_test_data):\n",
        "    mu_S = np.array(source_data.mean())\n",
        "    Sigma_S = np.array(source_data.cov())\n",
        "    mu_T_test = np.array(target_test_data.mean())\n",
        "    Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "    # Compute R1 (inverse of source portfolio Sharpe ratio)\n",
        "    R1 = compute_R1(optimize_source_portfolio(mu_S, Sigma_S), mu_S, Sigma_S)\n",
        "\n",
        "    # Compute R2 (Wasserstein-2 distance between source and target distributions)\n",
        "    R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "    return R1 + R2\n",
        "\n",
        "\n",
        "\n",
        "# Main experiment loop\n",
        "n_experiments = 10  # Reduced to 100 for faster testing; increase to 1000 for final results\n",
        "sharpe_ratios_transfer = []\n",
        "sharpe_ratios_direct = []\n",
        "transfer_risks = []\n",
        "\n",
        "# Download target data\n",
        "target_data = download_excess_returns(target_assets, target_start_date, test_end_date)\n",
        "target_train_data = target_data.loc[:target_end_date]  # Training data up to February 2020\n",
        "target_test_data = target_data.loc[test_start_date:]   # Testing data from February 2020 onwards\n",
        "\n",
        "# Compute target mean and covariance for training data\n",
        "mu_T = np.array(target_train_data.mean())\n",
        "Sigma_T = np.array(target_train_data.cov())\n",
        "\n",
        "for _ in range(n_experiments):\n",
        "    try:\n",
        "        # Randomly select source assets (10 random S&P 500 stocks)\n",
        "        source_assets = select_random_tickers(n=10)\n",
        "        print(\"Source Assets:\", source_assets)\n",
        "\n",
        "        # Generate source data\n",
        "        source_data = download_excess_returns(source_assets, source_start_date, source_end_date)\n",
        "        if not isinstance(source_data, pd.DataFrame) or source_data.shape[1] != 10:\n",
        "            print(\"Skipping experiment due to missing data for some source assets.\")\n",
        "            continue\n",
        "\n",
        "        # Optimize source portfolio (phi_S)\n",
        "        mu_S = np.array(source_data.mean())\n",
        "        Sigma_S = np.array(source_data.cov())\n",
        "        phi_S = optimize_source_portfolio(mu_S, Sigma_S)\n",
        "\n",
        "        # Optimize target portfolio with transfer learning (phi_T)\n",
        "        phi_T, sharpe_ratio_transfer = optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg)\n",
        "\n",
        "        # Optimize target portfolio with direct learning (phi_T_direct)\n",
        "        phi_T_direct = optimize_source_portfolio(mu_T, Sigma_T)\n",
        "\n",
        "        # Compute target mean and covariance for testing data\n",
        "        mu_T_test = np.array(target_test_data.mean())\n",
        "        Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "        # Compute Sharpe ratios on testing data (inverser compute_R1 pour obtenir le vrai ratio)\n",
        "        #sharpe_ratio_transfer_test = 1.0 / compute_R1(phi_S, mu_T_test, Sigma_T_test)\n",
        "        #sharpe_ratio_direct_test = 1.0 / compute_R1(phi_T_direct, mu_T_test, Sigma_T_test)\n",
        "        #phiTwhatever, sharpe_ratio_transfer_test = optimize_target_portfolio(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_transfer_test = optimize_target_portfolio2(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_direct_test= optimize_source_portfolio2(mu_T_test, Sigma_T_test)\n",
        "\n",
        "        # Compute transfer risk\n",
        "        R1 = compute_R1(phi_S, mu_S, Sigma_S)  # Correction : calculer R1 avec phi_S\n",
        "        R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "        transfer_risk = R1 + R2\n",
        "\n",
        "        # Store results\n",
        "        sharpe_ratios_transfer.append(sharpe_ratio_transfer)\n",
        "        sharpe_ratios_direct.append(sharpe_ratio_direct_test)\n",
        "        transfer_risks.append(transfer_risk)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in experiment {_}: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# Compute correlation between Sharpe ratios and transfer risks\n",
        "if len(sharpe_ratios_transfer) > 1 and len(transfer_risks) > 1:\n",
        "    correlation, p_value = pearsonr(sharpe_ratios_transfer, transfer_risks)\n",
        "    print(\"Correlation between Sharpe Ratio (Transfer) and Transfer Risk:\", correlation)\n",
        "    print(\"p-value:\", p_value)\n",
        "else:\n",
        "    print(\"Not enough experiments completed to compute correlation.\")\n",
        "    print(transfer_risks)\n",
        "    print(sharpe_ratios_transfer)\n",
        "\n",
        "    print(sharpe_ratios_direct)"
      ],
      "metadata": {
        "id": "S4dWBRTDmx5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Allemagne"
      ],
      "metadata": {
        "id": "xz_u4DcZDqfU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWecd46Vm-9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import pearsonr\n",
        "import random\n",
        "\n",
        "# Global parameters\n",
        "risk_free_rate = 0.05 / 252  # Daily risk-free rate (5% annual)\n",
        "source_start_date = \"2000-02-01\"\n",
        "source_end_date = \"2020-02-01\"\n",
        "target_start_date = \"2015-02-01\"\n",
        "target_end_date = \"2020-02-01\"\n",
        "test_start_date = \"2020-02-01\"\n",
        "test_end_date = \"2021-09-01\"\n",
        "lambda_reg = 0.2  # Regularization parameter\n",
        "\n",
        "# Fixed target assets (e.g., top 10 from DAX)\n",
        "\n",
        "target_assets= [\n",
        "    'SAP.DE', 'SIE.DE', 'DTE.DE', 'ALV.DE', 'MUV2.DE', 'MRK.DE', 'SHL.DE', 'BMW.DE', 'MBG.DE', 'VOW.DE']\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to get S&P 500 tickers with their addition date\n",
        "def get_sp500_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "\n",
        "    # Convert the \"Date added\" column to datetime\n",
        "    sp500_df['Date added'] = pd.to_datetime(sp500_df['Date added'], errors='coerce')\n",
        "\n",
        "    # Filter tickers added after the year 2000\n",
        "    sp500_df = sp500_df[sp500_df['Date added'] > datetime(2000, 1, 1)]\n",
        "\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers added after 2000\n",
        "def select_random_tickers(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "# Function to get S&P 500 tickers\n",
        "def get_sp500_tickers2():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers\n",
        "def select_random_tickers2(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "\n",
        "# Function to download excess returns\n",
        "def download_excess_returns(tickers, start_date, end_date):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        if not stock_data.empty:\n",
        "            price_column = 'Adj Close' if 'Adj Close' in stock_data.columns else 'Close'\n",
        "            stock_data['Excess Returns'] = stock_data[price_column].pct_change() - risk_free_rate\n",
        "            data[ticker] = stock_data['Excess Returns']\n",
        "    return pd.DataFrame(data).dropna()  # Drop rows with missing values\n",
        "\n",
        "\n",
        "# Function to optimize the source portfolio (phi_S)\n",
        "def optimize_source_portfolio(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_source.x\n",
        "\n",
        "def optimize_source_portfolio2(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_source.fun\n",
        "\n",
        "# Function to optimize the target portfolio with regularization (phi_T)\n",
        "def optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_target.x, -result_target.fun\n",
        "\n",
        "def optimize_target_portfolio2(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_target.fun\n",
        "\n",
        "# Function to compute transfer risk\n",
        "# Function to compute R1 (Transfer Risk)\n",
        "def compute_R1(phi, mu_T, Sigma_T):\n",
        "    numerator = np.dot(mu_T, phi)\n",
        "    denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "    ratio = numerator / denominator\n",
        "    R1 = 1.0 / ratio\n",
        "    return R1\n",
        "\n",
        "def compute_transfer_risk(phi_T, source_data, target_test_data):\n",
        "    mu_S = np.array(source_data.mean())\n",
        "    Sigma_S = np.array(source_data.cov())\n",
        "    mu_T_test = np.array(target_test_data.mean())\n",
        "    Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "    # Compute R1 (inverse of source portfolio Sharpe ratio)\n",
        "    R1 = compute_R1(optimize_source_portfolio(mu_S, Sigma_S), mu_S, Sigma_S)\n",
        "\n",
        "    # Compute R2 (Wasserstein-2 distance between source and target distributions)\n",
        "    R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "    return R1 + R2\n",
        "\n",
        "\n",
        "\n",
        "# Main experiment loop\n",
        "n_experiments = 10  # Reduced to 100 for faster testing; increase to 1000 for final results\n",
        "sharpe_ratios_transfer = []\n",
        "sharpe_ratios_direct = []\n",
        "transfer_risks = []\n",
        "\n",
        "# Download target data\n",
        "target_data = download_excess_returns(target_assets, target_start_date, test_end_date)\n",
        "target_train_data = target_data.loc[:target_end_date]  # Training data up to February 2020\n",
        "target_test_data = target_data.loc[test_start_date:]   # Testing data from February 2020 onwards\n",
        "\n",
        "# Compute target mean and covariance for training data\n",
        "mu_T = np.array(target_train_data.mean())\n",
        "Sigma_T = np.array(target_train_data.cov())\n",
        "\n",
        "for _ in range(n_experiments):\n",
        "    try:\n",
        "        # Randomly select source assets (10 random S&P 500 stocks)\n",
        "        source_assets = select_random_tickers(n=10)\n",
        "        print(\"Source Assets:\", source_assets)\n",
        "\n",
        "        # Generate source data\n",
        "        source_data = download_excess_returns(source_assets, source_start_date, source_end_date)\n",
        "        if not isinstance(source_data, pd.DataFrame) or source_data.shape[1] != 10:\n",
        "            print(\"Skipping experiment due to missing data for some source assets.\")\n",
        "            continue\n",
        "\n",
        "        # Optimize source portfolio (phi_S)\n",
        "        mu_S = np.array(source_data.mean())\n",
        "        Sigma_S = np.array(source_data.cov())\n",
        "        phi_S = optimize_source_portfolio(mu_S, Sigma_S)\n",
        "\n",
        "        # Optimize target portfolio with transfer learning (phi_T)\n",
        "        phi_T, sharpe_ratio_transfer = optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg)\n",
        "\n",
        "        # Optimize target portfolio with direct learning (phi_T_direct)\n",
        "        phi_T_direct = optimize_source_portfolio(mu_T, Sigma_T)\n",
        "\n",
        "        # Compute target mean and covariance for testing data\n",
        "        mu_T_test = np.array(target_test_data.mean())\n",
        "        Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "        # Compute Sharpe ratios on testing data (inverser compute_R1 pour obtenir le vrai ratio)\n",
        "        #sharpe_ratio_transfer_test = 1.0 / compute_R1(phi_S, mu_T_test, Sigma_T_test)\n",
        "        #sharpe_ratio_direct_test = 1.0 / compute_R1(phi_T_direct, mu_T_test, Sigma_T_test)\n",
        "        #phiTwhatever, sharpe_ratio_transfer_test = optimize_target_portfolio(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_transfer_test = optimize_target_portfolio2(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_direct_test= optimize_source_portfolio2(mu_T_test, Sigma_T_test)\n",
        "\n",
        "        # Compute transfer risk\n",
        "        R1 = compute_R1(phi_S, mu_S, Sigma_S)  # Correction : calculer R1 avec phi_S\n",
        "        R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "        transfer_risk = R1 + R2\n",
        "\n",
        "        # Store results\n",
        "        sharpe_ratios_transfer.append(sharpe_ratio_transfer)\n",
        "        sharpe_ratios_direct.append(sharpe_ratio_direct_test)\n",
        "        transfer_risks.append(transfer_risk)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in experiment {_}: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# Compute correlation between Sharpe ratios and transfer risks\n",
        "if len(sharpe_ratios_transfer) > 1 and len(transfer_risks) > 1:\n",
        "    correlation, p_value = pearsonr(sharpe_ratios_transfer, transfer_risks)\n",
        "    print(\"Correlation between Sharpe Ratio (Transfer) and Transfer Risk:\", correlation)\n",
        "    print(\"p-value:\", p_value)\n",
        "else:\n",
        "    print(\"Not enough experiments completed to compute correlation.\")\n",
        "    print(transfer_risks)\n",
        "    print(sharpe_ratios_transfer)\n",
        "\n",
        "    print(sharpe_ratios_direct)"
      ],
      "metadata": {
        "id": "b04THjQQm_Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Singapour"
      ],
      "metadata": {
        "id": "MIU7oFswyA04"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vV_yUw_XnI2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import pearsonr\n",
        "import random\n",
        "\n",
        "# Global parameters\n",
        "risk_free_rate = 0.05 / 252  # Daily risk-free rate (5% annual)\n",
        "source_start_date = \"2000-02-01\"\n",
        "source_end_date = \"2020-02-01\"\n",
        "target_start_date = \"2015-02-01\"\n",
        "target_end_date = \"2020-02-01\"\n",
        "test_start_date = \"2020-02-01\"\n",
        "test_end_date = \"2021-09-01\"\n",
        "lambda_reg = 0.2  # Regularization parameter\n",
        "\n",
        "# Fixed target assets (e.g., top 10 from SGX)\n",
        "\n",
        "target_assets= [\n",
        "    'D05.SI', 'O39.SI', 'U11.SI', 'Z74.SI', 'F34.SI', 'C6L.SI', 'J36.SI', 'S63.SI', 'C38U.SI', 'Y92.SI']\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to get S&P 500 tickers with their addition date\n",
        "def get_sp500_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "\n",
        "    # Convert the \"Date added\" column to datetime\n",
        "    sp500_df['Date added'] = pd.to_datetime(sp500_df['Date added'], errors='coerce')\n",
        "\n",
        "    # Filter tickers added after the year 2000\n",
        "    sp500_df = sp500_df[sp500_df['Date added'] > datetime(2000, 1, 1)]\n",
        "\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers added after 2000\n",
        "def select_random_tickers(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "# Function to get S&P 500 tickers\n",
        "def get_sp500_tickers2():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_df = tables[0]\n",
        "    return sp500_df[\"Symbol\"].tolist()\n",
        "\n",
        "# Function to randomly select n tickers\n",
        "def select_random_tickers2(n=10):\n",
        "    tickers = get_sp500_tickers()\n",
        "    if not tickers:\n",
        "        return \"Impossible to retrieve tickers.\"\n",
        "    return random.sample(tickers, n)\n",
        "\n",
        "\n",
        "\n",
        "# Function to download excess returns\n",
        "def download_excess_returns(tickers, start_date, end_date):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        if not stock_data.empty:\n",
        "            price_column = 'Adj Close' if 'Adj Close' in stock_data.columns else 'Close'\n",
        "            stock_data['Excess Returns'] = stock_data[price_column].pct_change() - risk_free_rate\n",
        "            data[ticker] = stock_data['Excess Returns']\n",
        "    return pd.DataFrame(data).dropna()  # Drop rows with missing values\n",
        "\n",
        "\n",
        "# Function to optimize the source portfolio (phi_S)\n",
        "def optimize_source_portfolio(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_source.x\n",
        "\n",
        "def optimize_source_portfolio2(mu_S, Sigma_S):\n",
        "    def sharpe_ratio_source(phi, mu_S, Sigma_S):\n",
        "        numerator = np.dot(mu_S, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_S, phi)))\n",
        "        return -numerator / denominator  # Negative because we minimize\n",
        "\n",
        "    result_source = minimize(sharpe_ratio_source, np.ones(len(mu_S)) / len(mu_S), args=(mu_S, Sigma_S),\n",
        "                             method='SLSQP', bounds=[(0, 1) for _ in range(len(mu_S))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_source.fun\n",
        "\n",
        "# Function to optimize the target portfolio with regularization (phi_T)\n",
        "def optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return result_target.x, -result_target.fun\n",
        "\n",
        "def optimize_target_portfolio2(mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "    def regularized_sharpe_ratio(phi, mu_T, Sigma_T, phi_S, lambda_reg):\n",
        "        numerator = np.dot(mu_T, phi)\n",
        "        denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "        sharpe_ratio = numerator / denominator\n",
        "        regularization = lambda_reg * np.linalg.norm(phi_S - phi) ** 2  # Regularization term\n",
        "        return -(sharpe_ratio - regularization)  # Negative because we minimize\n",
        "\n",
        "    result_target = minimize(regularized_sharpe_ratio, np.ones(len(mu_T)) / len(mu_T),\n",
        "                             args=(mu_T, Sigma_T, phi_S, lambda_reg), method='SLSQP',\n",
        "                             bounds=[(0, 1) for _ in range(len(mu_T))],\n",
        "                             constraints=[{'type': 'eq', 'fun': lambda phi: np.sum(phi) - 1}])\n",
        "    return -result_target.fun\n",
        "\n",
        "# Function to compute transfer risk\n",
        "# Function to compute R1 (Transfer Risk)\n",
        "def compute_R1(phi, mu_T, Sigma_T):\n",
        "    numerator = np.dot(mu_T, phi)\n",
        "    denominator = np.sqrt(np.dot(phi.T, np.dot(Sigma_T, phi)))\n",
        "    ratio = numerator / denominator\n",
        "    R1 = 1.0 / ratio\n",
        "    return R1\n",
        "\n",
        "def compute_transfer_risk(phi_T, source_data, target_test_data):\n",
        "    mu_S = np.array(source_data.mean())\n",
        "    Sigma_S = np.array(source_data.cov())\n",
        "    mu_T_test = np.array(target_test_data.mean())\n",
        "    Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "    # Compute R1 (inverse of source portfolio Sharpe ratio)\n",
        "    R1 = compute_R1(optimize_source_portfolio(mu_S, Sigma_S), mu_S, Sigma_S)\n",
        "\n",
        "    # Compute R2 (Wasserstein-2 distance between source and target distributions)\n",
        "    R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "    return R1 + R2\n",
        "\n",
        "\n",
        "\n",
        "# Main experiment loop\n",
        "n_experiments = 10  # Reduced to 100 for faster testing; increase to 1000 for final results\n",
        "sharpe_ratios_transfer = []\n",
        "sharpe_ratios_direct = []\n",
        "transfer_risks = []\n",
        "\n",
        "# Download target data\n",
        "target_data = download_excess_returns(target_assets, target_start_date, test_end_date)\n",
        "target_train_data = target_data.loc[:target_end_date]  # Training data up to February 2020\n",
        "target_test_data = target_data.loc[test_start_date:]   # Testing data from February 2020 onwards\n",
        "\n",
        "# Compute target mean and covariance for training data\n",
        "mu_T = np.array(target_train_data.mean())\n",
        "Sigma_T = np.array(target_train_data.cov())\n",
        "\n",
        "for _ in range(n_experiments):\n",
        "    try:\n",
        "        # Randomly select source assets (10 random S&P 500 stocks)\n",
        "        source_assets = select_random_tickers(n=10)\n",
        "        print(\"Source Assets:\", source_assets)\n",
        "\n",
        "        # Generate source data\n",
        "        source_data = download_excess_returns(source_assets, source_start_date, source_end_date)\n",
        "        if not isinstance(source_data, pd.DataFrame) or source_data.shape[1] != 10:\n",
        "            print(\"Skipping experiment due to missing data for some source assets.\")\n",
        "            continue\n",
        "\n",
        "        # Optimize source portfolio (phi_S)\n",
        "        mu_S = np.array(source_data.mean())\n",
        "        Sigma_S = np.array(source_data.cov())\n",
        "        phi_S = optimize_source_portfolio(mu_S, Sigma_S)\n",
        "\n",
        "        # Optimize target portfolio with transfer learning (phi_T)\n",
        "        phi_T, sharpe_ratio_transfer = optimize_target_portfolio(mu_T, Sigma_T, phi_S, lambda_reg)\n",
        "\n",
        "        # Optimize target portfolio with direct learning (phi_T_direct)\n",
        "        phi_T_direct = optimize_source_portfolio(mu_T, Sigma_T)\n",
        "\n",
        "        # Compute target mean and covariance for testing data\n",
        "        mu_T_test = np.array(target_test_data.mean())\n",
        "        Sigma_T_test = np.array(target_test_data.cov())\n",
        "\n",
        "        # Compute Sharpe ratios on testing data (inverser compute_R1 pour obtenir le vrai ratio)\n",
        "        #sharpe_ratio_transfer_test = 1.0 / compute_R1(phi_S, mu_T_test, Sigma_T_test)\n",
        "        #sharpe_ratio_direct_test = 1.0 / compute_R1(phi_T_direct, mu_T_test, Sigma_T_test)\n",
        "        #phiTwhatever, sharpe_ratio_transfer_test = optimize_target_portfolio(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_transfer_test = optimize_target_portfolio2(mu_T_test, Sigma_T_test, phi_S, lambda_reg)\n",
        "        sharpe_ratio_direct_test= optimize_source_portfolio2(mu_T_test, Sigma_T_test)\n",
        "\n",
        "        # Compute transfer risk\n",
        "        R1 = compute_R1(phi_S, mu_S, Sigma_S)  # Correction : calculer R1 avec phi_S\n",
        "        R2 = np.linalg.norm(mu_S - mu_T_test) + np.linalg.norm(Sigma_S - Sigma_T_test, ord=2)\n",
        "        transfer_risk = R1 + R2\n",
        "\n",
        "        # Store results\n",
        "        sharpe_ratios_transfer.append(sharpe_ratio_transfer)\n",
        "        sharpe_ratios_direct.append(sharpe_ratio_direct_test)\n",
        "        transfer_risks.append(transfer_risk)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in experiment {_}: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# Compute correlation between Sharpe ratios and transfer risks\n",
        "if len(sharpe_ratios_transfer) > 1 and len(transfer_risks) > 1:\n",
        "    correlation, p_value = pearsonr(sharpe_ratios_transfer, transfer_risks)\n",
        "    print(\"Correlation between Sharpe Ratio (Transfer) and Transfer Risk:\", correlation)\n",
        "    print(\"p-value:\", p_value)\n",
        "else:\n",
        "    print(\"Not enough experiments completed to compute correlation.\")\n",
        "    print(transfer_risks)\n",
        "    print(sharpe_ratios_transfer)\n",
        "\n",
        "    print(sharpe_ratios_direct)"
      ],
      "metadata": {
        "id": "ZWxxGDainJBm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}